{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN tension prediction for SUPERball\n",
    "### &nbsp; &nbsp; &nbsp; Chiara Ercolani\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cross validation stuff added. This implementation trains the neural network with a big dataset and then saves it.\n",
    "\n",
    "Dataset:2017-10-24_BiggerBetterData_1kHz.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combine data from all three datasets\n",
    "dataset1=scipy.io.loadmat('2018-02-08_Biggester_slow_data.mat')\n",
    "#dataset2=scipy.io.loadmat('2018-02-01_17_Just_motor22.mat')\n",
    "#dataset3=scipy.io.loadmat('2018-01-30_16-07_Better_ReallyBig_Data.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1127580, 24)\n"
     ]
    }
   ],
   "source": [
    "D=23   #number of features recorded at each time step(sequence length) [all motors except for motor 22]\n",
    "effort=dataset1.get('effort')\n",
    "#effort=np.concatenate((effort,dataset2.get('effort')))\n",
    "#effort=np.concatenate((effort,dataset3.get('effort')))\n",
    "\n",
    "#Exclude motor 22 from the features\n",
    "effort1=effort[:,0:21]\n",
    "#Also want to include the output (@24)\n",
    "effort2=effort[:,22:25]\n",
    "effortFeatures=np.concatenate((effort1,effort2),axis=1)\n",
    "print(effortFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cross validation parameters\n",
    "#Number of cross validation boxes\n",
    "K=10\n",
    "#Number of samples in each box\n",
    "set_length=int(len(effort)/K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_len= 1014822  batch_size= 150  batch_len= 6765  D= 23 H1= 230\n"
     ]
    }
   ],
   "source": [
    "#Various paramenters\n",
    "\n",
    "#CHANGING BATCH SIZE\n",
    "batch_size = 150\n",
    "batch_len =(set_length*(K-1))//batch_size\n",
    "margin=0.05                                     # accuracy margin\n",
    "lr=0.0003                                       # learning rate (originally 0.0003)\n",
    "epochNumber=100                                  # Number of epochs\n",
    "                                \n",
    "H1= 10*D                                         # size of hidden state\n",
    "radius=1/0.0155\n",
    "\n",
    "print('train_data_len=',set_length*(K-1),' batch_size=',batch_size,' batch_len=',\n",
    "      batch_len,' D=',D,'H1=',H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wx1= (23, 230)\n",
      "Wy= (230, 1)\n",
      "bh1= (1, 230)\n",
      "by= (1, 1)\n"
     ]
    }
   ],
   "source": [
    "#Input shape: (num_samples,seq_length,input_dimension)\n",
    "#Output shape: (num_samples, target)\n",
    "Xin= tf.placeholder(tf.float32,shape=[batch_size,D],name='Xin')\n",
    "Ytarget = tf.placeholder(tf.float32,shape=[batch_size,1],name='Ytarget')\n",
    "\n",
    "#Xavier initialization for weights \n",
    "#http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "Wx1 = tf.get_variable(\"Wx1\", shape=[D, H1],initializer=tf.contrib.layers.xavier_initializer()); print('Wx1=',Wx1.get_shape())\n",
    "#Wx2 = tf.get_variable(\"Wx2\", shape=[H1, H2],initializer=tf.contrib.layers.xavier_initializer()); print('Wx2=',Wx2.get_shape())\n",
    "#Wx3 = tf.get_variable(\"Wx3\", shape=[H2, H3],initializer=tf.contrib.layers.xavier_initializer()); print('Wx3=',Wx2.get_shape())\n",
    "Wy = tf.get_variable(\"Wy\", shape=[H1, 1],initializer=tf.contrib.layers.xavier_initializer()); print('Wy=',Wy.get_shape())\n",
    "\n",
    "#Biases initalized to 0\n",
    "bh1 = tf.Variable(tf.zeros([1,H1]),name=\"bh1\"); print('bh1=',bh1.get_shape())\n",
    "#bh2 = tf.Variable(tf.zeros([1,H2]),name=\"bh2\"); print('bh2=',bh2.get_shape())\n",
    "#bh3 = tf.Variable(tf.zeros([1,H3]),name=\"bh3\"); print('bh3=',bh3.get_shape())\n",
    "by = tf.Variable(tf.zeros([1,1]),name=\"by\"); print('by=',by.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ypredicted= (150, 1)\n",
      "Ytarget= (150, 1)\n"
     ]
    }
   ],
   "source": [
    "# NN implementation with ReLU function and one hidden layer\n",
    "\n",
    "h_1=tf.nn.relu(tf.matmul(Xin,Wx1)+bh1)\n",
    "#h_2=tf.nn.relu(tf.matmul(h_1,Wx2)+bh2)\n",
    "#h_3=tf.nn.relu(tf.matmul(h_2,Wx3)+bh3)\n",
    "y_=tf.add(tf.matmul(h_1,Wy),by,name='y_')\n",
    "\n",
    "print('Ypredicted=',y_.get_shape())\n",
    "print('Ytarget=',Ytarget.get_shape())\n",
    "\n",
    "#Mean Absolute Error cost function\n",
    "\n",
    "cost=tf.reduce_mean(tf.abs(tf.subtract(y_, Ytarget)))\n",
    "\n",
    "#Optimizer used to implement backpropagation\n",
    "#Adding decayed learning rate\n",
    "#batch_var=tf.Variable(0)\n",
    "#learning_rate=tf.train.exponential_decay(lr,batch_var*batch_size,len(train_features),0.95,staircase=True)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step=batch_var)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy measure set\n"
     ]
    }
   ],
   "source": [
    "#ACCURACY INTERVAL\n",
    "lower_bound =tf.greater_equal(y_,tf.subtract(Ytarget,margin))\n",
    "upper_bound= tf.less_equal(y_,tf.add(Ytarget,margin))\n",
    "correct=tf.equal(lower_bound,upper_bound)\n",
    "#correct=tf.equal(cast_out_data,cast_out_pred)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "print('Accuracy measure set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION ITERATION 0\n",
      "Epoch: 0, Cost: 8.007299423217773, Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "rmse=[]\n",
    "rmse_motor=[]\n",
    "for k in range(0,K):\n",
    "    print(\"CROSS VALIDATION ITERATION\",k)\n",
    "    # Initializing session    \n",
    "    sess = tf.Session()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    train_features=[]\n",
    "    train_features2=[]\n",
    "    test_features=[]\n",
    "    train_target=[]\n",
    "    test_target=[]\n",
    "    \n",
    "    \n",
    "    #Define saver object to save NN\n",
    "    locals()[\"saver_\"+str(k)]=tf.train.Saver()\n",
    "    train_features= effortFeatures[0:k*set_length,0:D]\n",
    "    train_features=np.concatenate((train_features,effortFeatures[(k+1)*set_length:K*set_length,0:D]))\n",
    "    \n",
    "    test_features= effortFeatures[k*set_length:(k+1)*set_length,0:D]\n",
    "                               \n",
    "\n",
    "    train_target= -effortFeatures[0:k*set_length,D]\n",
    "    train_target=np.concatenate((train_target,-effortFeatures[(k+1)*set_length:K*set_length,D]))\n",
    "    \n",
    "    test_target=-effortFeatures[k*set_length:(k+1)*set_length,D]\n",
    "                           \n",
    "                               \n",
    "    #Reshaping data\n",
    "    train_features=np.asarray(train_features)\n",
    "    train_features=np.reshape(train_features,[len(train_features),D])\n",
    "    test_features=np.asarray(test_features)\n",
    "    test_features=np.reshape(test_features,[len(test_features),D])\n",
    "    train_target=np.asarray(train_target)\n",
    "    train_target=np.reshape(train_target,[len(train_target),1])\n",
    "    test_target=np.asarray(test_target) \n",
    "    test_target=np.reshape(test_target,[len(test_target),1]) \n",
    "    \n",
    "    #Regularize to radius\n",
    "    train_target=train_target*radius\n",
    "    test_target=test_target*radius\n",
    "\n",
    "    costs = []\n",
    "    accuracies=[]\n",
    "\n",
    "    #setting up training\n",
    "    for epoch in range(epochNumber):\n",
    "        total_batch=int(len(train_features)/batch_size)\n",
    "        for i in range(total_batch):   \n",
    "            inp_data= train_features[i*batch_size:(i+1)*batch_size,:]\n",
    "            out_data= train_target[i*batch_size:(i+1)*batch_size,:]\n",
    "            _,acc,c = sess.run([optimizer,accuracy,cost],feed_dict={Xin: inp_data, Ytarget: out_data})\n",
    "        print(\"Epoch: {}, Cost: {}, Accuracy: {}\".format(epoch,c,acc))\n",
    "        costs.append(c)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    #Save session after training\n",
    "    saver_path=locals()[\"saver_\"+str(k)].save(sess,'results/NN_tension_pred_saver',global_step=k)\n",
    "\n",
    "    print (\"\\nTraining complete!\")\n",
    "\n",
    "    predicted=[]\n",
    "    test_batch=int(len(test_features)/batch_size)\n",
    "\n",
    "    #Test prediction with test dataset\n",
    "    for j in range (test_batch):    \n",
    "        inp_data = test_features[j*batch_size:(j+1)*batch_size,:]\n",
    "        pred = sess.run(y_, feed_dict={Xin: inp_data})\n",
    "        predicted=np.append(predicted,pred)\n",
    "\n",
    "    #Plot prediction against real data from test dataset\n",
    "    x=np.arange(0,len(test_target[0:(batch_size*test_batch)]))\n",
    "    y_pred= predicted\n",
    "    y_tar= test_target[0:(batch_size*test_batch)]\n",
    "    #y3=-test_features[:,21]*0.0175/0.008\n",
    "    y_motor=effort[k*set_length:k*set_length+(batch_size*test_batch),21]/0.0155*2.5\n",
    "\n",
    "    fig= plt.figure(figsize=(20,10))\n",
    "    ax1=plt.subplot(211)\n",
    "    ax1.plot(x,y_pred,'b',label='Predictions')\n",
    "    ax1.plot(x,y_tar,'r',label='Targets')\n",
    "    ax1.plot(x,y_motor,'g',label='Motor 22')\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.title('Prediction using 24 motor torques as features')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot cost function and accuracy\n",
    "    x=np.arange(0,epochNumber)\n",
    "    y= costs\n",
    "\n",
    "    fig= plt.figure(figsize=(10,10))\n",
    "    ax1=plt.subplot(211)\n",
    "    ax1.plot(x,y,'b',label='Cost')\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.title('Cost Fuction during Training')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Plot cost function and accuracy\n",
    "    x=np.arange(0,epochNumber)\n",
    "    y= accuracies\n",
    "\n",
    "    fig= plt.figure(figsize=(10,10))\n",
    "    ax1=plt.subplot(211)\n",
    "    ax1.plot(x,y,'r',label='Accuracy')\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "\n",
    "    plt.title('Accuracy during Training')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    rmse=np.append(rmse,sqrt(mean_squared_error(y_pred,y_tar)))\n",
    "    rmse_motor=np.append(rmse_motor,sqrt(mean_squared_error(y_motor,y_tar)))\n",
    "    print(\"And now costs for iteration \", k)\n",
    "    print(costs)\n",
    "    print(\"And now accuracies for iteration \", k)\n",
    "    print(accuracies)\n",
    "    print(\"Root mean squared error of prediction\",rmse[k])\n",
    "    print(\"Root mean squared error of motor\",rmse_motor[k])\n",
    "\n",
    "    sess.close()\n",
    "print(rmse)\n",
    "print(rmse_motor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ADAM\n",
    "#batch_size=800, lr=0.0001, K=10, H=4*D, epoch_number=100 \n",
    "#[ 0.50253549  0.43781634  0.32040607  0.32329862  0.36412889  0.35812448\n",
    "#  0.27207648  0.43003502  0.32162693  0.40849026]\n",
    "#[ 0.70579643  0.64966416  0.50766536  0.50934368  0.59895063  0.61945065\n",
    "#  0.48574498  0.67014488  0.48967202  0.61927773]\n",
    "#\n",
    "#batch_size=100, lr=0.0003, K=10, H=5*D, epoch_number=100\n",
    "#[ 0.46218572  0.44220819  0.39805842  0.41903569  0.3219875   0.3759407\n",
    "#  0.35630052  0.46268236  0.37257996  0.54600813]\n",
    "#[ 0.70500703  0.64769705  0.50612557  0.50971187  0.59769723  0.61814086\n",
    "#  0.48427049  0.66855334  0.4923125   0.61763899]\n",
    "\n",
    "\n",
    "#batch_size=500, lr=0.0003, K=10, H=5*D, epoch_number=100\n",
    "#[ 0.45112949  0.41166623  0.3488705   0.33144462  0.30662881  0.37951444\n",
    "#  0.26394875  0.4951899   0.29257544  0.48440297]\n",
    "#[ 0.70496692  0.64826301  0.50657057  0.50966813  0.59798115  0.61856779\n",
    "#  0.48468212  0.66901232  0.49050164  0.61812131]\n",
    "\n",
    "#batch_size=500, lr=0.0003, K=10, H=5*D, epoch_number=300\n",
    "#[ 0.46730755  0.39162348  0.33621428  0.35487699  0.33413626  0.4338099\n",
    "#  0.338667    0.46846256  0.3382114   0.55634947]\n",
    "#[ 0.70496692  0.64826301  0.50657057  0.50966813  0.59798115  0.61856779\n",
    "#  0.48468212  0.66901232  0.49050164  0.61812131]\n",
    "\n",
    "##batch_size=200, lr=0.00005, K=10, H=15*D, epoch_number=100\n",
    "#[ 0.43956565  0.39496273  0.30556959  0.32533406  0.33855176  0.37363014\n",
    "#  0.28639368  0.46719669  0.31364863  0.42626489]\n",
    "#[ 0.70492231  0.64797778  0.50634846  0.50969095  0.59783157  0.61834494\n",
    "#  0.48447538  0.66878822  0.49126617  0.6178877 ]\n",
    "\n",
    "#batch_size=200, lr=0.00005, K=10, H=20*D, epoch_number=100\n",
    "#[ 0.45373054  0.39110336  0.32326501  0.33262925  0.35039483  0.40166448\n",
    "#  0.25592091  0.4547832   0.30091716  0.44751162]\n",
    "#[ 0.70492231  0.64797778  0.50634846  0.50969095  0.59783157  0.61834494\n",
    "#  0.48447538  0.66878822  0.49126617  0.6178877 ]\n",
    "\n",
    "\n",
    "#MINI BATCH GD\n",
    "#batch_size=100, lr=0.00001, K=10, H=15*D, epoch_number=200\n",
    "#[ 0.48721713,  0.43863498,  0.33641318,  0.33274214,  0.36547824,  0.35071493,\n",
    "#  0.27092393,  0.42188746,  0.33233472,  0.42640026]\n",
    "#[ 0.70500703  0.64769705  0.50612557  0.50971187  0.59769723  0.61814086\n",
    "#  0.48427049  0.66855334  0.4923125   0.61763899]\n",
    "\n",
    "#batch_size=50, lr=0.00001, K=10, H=15*D, epoch_number=100\n",
    "#[ 0.48799102  0.45342051  0.33284949  0.32443756  0.35144045  0.34472716\n",
    "#  0.28091073  0.42006393  0.33825867  0.42149133]\n",
    "#[ 0.70509088  0.64755575  0.50601392  0.50971825  0.59763682  0.61804062\n",
    "#  0.48416542  0.66842852  0.49275822  0.61751091]\n",
    "\n",
    "\n",
    "#batch_size=50, lr=0.00001, K=10, H=18*D, epoch_number=100\n",
    "#[ 0.49460297  0.45634525  0.32905642  0.32753956  0.35563032  0.35412292\n",
    "#  0.27835449  0.42779808  0.33047787  0.41999861]\n",
    "#[ 0.70509088  0.64755575  0.50601392  0.50971825  0.59763682  0.61804062\n",
    "#  0.48416542  0.66842852  0.49275822  0.61751091]\n",
    "\n",
    "#batch_size=50, lr=0.00001, K=10, H=10*D, epoch_number=100\n",
    "#[ 0.48540827  0.44815957  0.33539943  0.33215633  0.37294727  0.34524683\n",
    "#  0.26910294  0.42505782  0.34029048  0.42350589]\n",
    "#[ 0.70509088  0.64755575  0.50601392  0.50971825  0.59763682  0.61804062\n",
    "#  0.48416542  0.66842852  0.49275822  0.61751091]\n",
    "\n",
    "\n",
    "#batch_size=50, lr=0.00001, K=7, H=15*D, epoch_number=100\n",
    "[ 0.49522226  0.3746474   0.35643217  0.33235772  0.33975522  0.34873753\n",
    "  0.41965114]\n",
    "[ 0.69181523  0.56609945  0.54216891  0.58965845  0.54702666  0.5808668\n",
    "  0.59657655]\n",
    "\n",
    "#batch_size=50, lr=0.000005, K=10, H=15*D, epoch_number=200\n",
    "[ 0.48740707  0.46531014  0.33256863  0.33262834  0.35801402  0.34543959\n",
    "  0.28328477  0.41193634  0.33103235  0.42206953]\n",
    "[ 0.70509088  0.64755575  0.50601392  0.50971825  0.59763682  0.61804062\n",
    "  0.48416542  0.66842852  0.49275822  0.61751091]\n",
    "\n",
    "\n",
    "GOOD ONE\n",
    "#batch_size=50, lr=0.000005, K=10, H=15*D, epoch_number=300\n",
    "# [0.47976632  0.45696714  0.33366897  0.3296227   0.35630887  0.33606939\n",
    "#  0.28331008  0.41057156  0.32990863  0.42158835]\n",
    "#[ 0.70509088  0.64755575  0.50601392  0.50971825  0.59763682  0.61804062\n",
    "#  0.48416542  0.66842852  0.49275822  0.61751091]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ciao=[ 0.70509088,  0.64755575 , 0.50601392 , 0.50971825 , 0.59763682,  0.61804062,\n",
    "  0.48416542 , 0.66842852 , 0.49275822 , 0.61751091]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584691931\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(ciao))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1127580, 25)\n",
      "(1127580, 21)\n",
      "(1127580, 2)\n",
      "(1127580, 23)\n"
     ]
    }
   ],
   "source": [
    "effort=dataset1.get('effort')\n",
    "#effort=np.concatenate((effort,dataset2.get('effort')))\n",
    "#effort=np.concatenate((effort,dataset3.get('effort')))\n",
    "print(effort.shape)\n",
    "effort1=effort[:,0:21]\n",
    "effort2=effort[:,21:D]\n",
    "effort=np.concatenate((effort1,effort2),axis=1)\n",
    "print(effort1.shape)\n",
    "print(effort2.shape)\n",
    "print(effort.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
