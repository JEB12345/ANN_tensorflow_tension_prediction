{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN tension prediction for SUPERball\n",
    "### &nbsp; &nbsp; &nbsp; Chiara Ercolani\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Biases initialized to small value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine data from all three datasets\n",
    "dataset1=scipy.io.loadmat('2018-02-08_Biggester_slow_data.mat')\n",
    "#dataset2=scipy.io.loadmat('2018-02-01_17_Just_motor22.mat')\n",
    "#dataset3=scipy.io.loadmat('2018-01-30_16-07_Better_ReallyBig_Data.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1127580, 24)\n"
     ]
    }
   ],
   "source": [
    "D=23   #number of features recorded at each time step(sequence length) [all motors except for motor 22]\n",
    "effort=dataset1.get('effort')\n",
    "#effort=np.concatenate((effort,dataset2.get('effort')))\n",
    "#effort=np.concatenate((effort,dataset3.get('effort')))\n",
    "\n",
    "#Exclude motor 22 from the features\n",
    "effort1=effort[:,0:21]\n",
    "#Also want to include the output (@24)\n",
    "effort2=effort[:,22:25]\n",
    "effortFeatures=np.concatenate((effort1,effort2),axis=1)\n",
    "print(effortFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cross validation parameters\n",
    "#Number of cross validation boxes\n",
    "K=10\n",
    "#Number of samples in each box\n",
    "set_length=int(len(effort)/K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_len= 1014822  batch_size= 150  batch_len= 6765  D= 23 H1= 345\n"
     ]
    }
   ],
   "source": [
    "#Various paramenters\n",
    "\n",
    "#CHANGING BATCH SIZE\n",
    "batch_size = 150\n",
    "batch_len =(set_length*(K-1))//batch_size\n",
    "margin=0.05                                     # accuracy margin\n",
    "lr=0.00005                                      # learning rate (originally 0.0003)\n",
    "epochNumber=100                                 # Number of epochs\n",
    "desiredAccuracy=0.7\n",
    "                                \n",
    "H1= 15*D                                         # size of hidden state\n",
    "radius=1/0.0155\n",
    "\n",
    "print('train_data_len=',set_length*(K-1),' batch_size=',batch_size,' batch_len=',\n",
    "      batch_len,' D=',D,'H1=',H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wx1= (23, 345)\n",
      "Wy= (345, 1)\n",
      "bh1= (1, 345)\n",
      "by= (1, 1)\n"
     ]
    }
   ],
   "source": [
    "#Input shape: (num_samples,seq_length,input_dimension)\n",
    "#Output shape: (num_samples, target)\n",
    "Xin= tf.placeholder(tf.float32,shape=[batch_size,D],name='Xin')\n",
    "Ytarget = tf.placeholder(tf.float32,shape=[batch_size,1],name='Ytarget')\n",
    "\n",
    "#Xavier initialization for weights \n",
    "#http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "Wx1 = tf.get_variable(\"Wx1\", shape=[D, H1],initializer=tf.contrib.layers.xavier_initializer()); print('Wx1=',Wx1.get_shape())\n",
    "#Wx2 = tf.get_variable(\"Wx2\", shape=[H1, H2],initializer=tf.contrib.layers.xavier_initializer()); print('Wx2=',Wx2.get_shape())\n",
    "#Wx3 = tf.get_variable(\"Wx3\", shape=[H2, H3],initializer=tf.contrib.layers.xavier_initializer()); print('Wx3=',Wx2.get_shape())\n",
    "Wy = tf.get_variable(\"Wy\", shape=[H1, 1],initializer=tf.contrib.layers.xavier_initializer()); print('Wy=',Wy.get_shape())\n",
    "\n",
    "#Biases initalized to 0\n",
    "bh1 = tf.Variable(tf.ones([1,H1])*0.001,name=\"bh1\"); print('bh1=',bh1.get_shape())\n",
    "#bh2 = tf.Variable(tf.zeros([1,H2]),name=\"bh2\"); print('bh2=',bh2.get_shape())\n",
    "#bh3 = tf.Variable(tf.zeros([1,H3]),name=\"bh3\"); print('bh3=',bh3.get_shape())\n",
    "by = tf.Variable(tf.ones([1,1])*0.001,name=\"by\"); print('by=',by.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ypredicted= (150, 1)\n",
      "Ytarget= (150, 1)\n"
     ]
    }
   ],
   "source": [
    "# NN implementation with ReLU function and one hidden layer\n",
    "\n",
    "h_1=tf.nn.relu(tf.matmul(Xin,Wx1)+bh1)\n",
    "#h_2=tf.nn.relu(tf.matmul(h_1,Wx2)+bh2)\n",
    "#h_3=tf.nn.relu(tf.matmul(h_2,Wx3)+bh3)\n",
    "y_=tf.add(tf.matmul(h_1,Wy),by,name='y_')\n",
    "\n",
    "print('Ypredicted=',y_.get_shape())\n",
    "print('Ytarget=',Ytarget.get_shape())\n",
    "\n",
    "#Mean Absolute Error cost function\n",
    "\n",
    "cost=tf.reduce_mean(tf.abs(tf.subtract(y_, Ytarget)))\n",
    "\n",
    "#Optimizer used to implement backpropagation\n",
    "#Adding decayed learning rate\n",
    "#batch_var=tf.Variable(0)\n",
    "#learning_rate=tf.train.exponential_decay(lr,batch_var*batch_size,len(train_features),0.95,staircase=True)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy measure set\n"
     ]
    }
   ],
   "source": [
    "#ACCURACY INTERVAL\n",
    "lower_bound =tf.greater_equal(y_,tf.subtract(Ytarget,margin))\n",
    "upper_bound= tf.less_equal(y_,tf.add(Ytarget,margin))\n",
    "correct=tf.equal(lower_bound,upper_bound)\n",
    "#correct=tf.equal(cast_out_data,cast_out_pred)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "print('Accuracy measure set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION ITERATION 0\n",
      "Epoch: 0, Cost: 94.49454498291016, Accuracy: 0.0\n",
      "Epoch: 1, Cost: 33.3946647644043, Accuracy: 0.0\n",
      "Epoch: 2, Cost: 40.334754943847656, Accuracy: 0.0\n",
      "Epoch: 3, Cost: 37.2031135559082, Accuracy: 0.0\n",
      "Epoch: 4, Cost: 34.07879638671875, Accuracy: 0.0\n",
      "Epoch: 5, Cost: 31.444053649902344, Accuracy: 0.0\n",
      "Epoch: 6, Cost: 29.333499908447266, Accuracy: 0.0\n",
      "Epoch: 7, Cost: 26.858158111572266, Accuracy: 0.0\n",
      "Epoch: 8, Cost: 24.702356338500977, Accuracy: 0.0\n",
      "Epoch: 9, Cost: 22.811443328857422, Accuracy: 0.0\n",
      "Epoch: 10, Cost: 21.12898063659668, Accuracy: 0.0\n",
      "Epoch: 11, Cost: 19.676929473876953, Accuracy: 0.0\n",
      "Epoch: 12, Cost: 18.484638214111328, Accuracy: 0.0\n",
      "Epoch: 13, Cost: 17.481679916381836, Accuracy: 0.0\n",
      "Epoch: 14, Cost: 16.597583770751953, Accuracy: 0.0\n",
      "Epoch: 15, Cost: 15.862868309020996, Accuracy: 0.0\n",
      "Epoch: 16, Cost: 15.280301094055176, Accuracy: 0.0\n",
      "Epoch: 17, Cost: 14.734557151794434, Accuracy: 0.0\n",
      "Epoch: 18, Cost: 14.265751838684082, Accuracy: 0.0\n",
      "Epoch: 19, Cost: 13.835418701171875, Accuracy: 0.0\n",
      "Epoch: 20, Cost: 13.438855171203613, Accuracy: 0.0\n",
      "Epoch: 21, Cost: 13.104121208190918, Accuracy: 0.0\n",
      "Epoch: 22, Cost: 12.777511596679688, Accuracy: 0.0\n",
      "Epoch: 23, Cost: 12.478858947753906, Accuracy: 0.0\n",
      "Epoch: 24, Cost: 12.188310623168945, Accuracy: 0.0\n",
      "Epoch: 25, Cost: 11.961731910705566, Accuracy: 0.0\n",
      "Epoch: 26, Cost: 11.732906341552734, Accuracy: 0.0\n",
      "Epoch: 27, Cost: 11.488160133361816, Accuracy: 0.0\n",
      "Epoch: 28, Cost: 11.229488372802734, Accuracy: 0.0\n",
      "Epoch: 29, Cost: 10.970067977905273, Accuracy: 0.0\n",
      "Epoch: 30, Cost: 10.74084186553955, Accuracy: 0.0\n",
      "Epoch: 31, Cost: 10.54339599609375, Accuracy: 0.0\n",
      "Epoch: 32, Cost: 10.354194641113281, Accuracy: 0.0\n",
      "Epoch: 33, Cost: 10.168211936950684, Accuracy: 0.0\n",
      "Epoch: 34, Cost: 9.992262840270996, Accuracy: 0.0\n",
      "Epoch: 35, Cost: 9.84620189666748, Accuracy: 0.0\n",
      "Epoch: 36, Cost: 9.730718612670898, Accuracy: 0.0\n",
      "Epoch: 37, Cost: 9.61894702911377, Accuracy: 0.0\n",
      "Epoch: 38, Cost: 9.494485855102539, Accuracy: 0.0\n",
      "Epoch: 39, Cost: 9.372384071350098, Accuracy: 0.0\n",
      "Epoch: 40, Cost: 9.264013290405273, Accuracy: 0.0\n",
      "Epoch: 41, Cost: 9.137384414672852, Accuracy: 0.0\n",
      "Epoch: 42, Cost: 9.034226417541504, Accuracy: 0.0\n",
      "Epoch: 43, Cost: 8.959863662719727, Accuracy: 0.0\n",
      "Epoch: 44, Cost: 8.890979766845703, Accuracy: 0.0\n",
      "Epoch: 45, Cost: 8.820352554321289, Accuracy: 0.0\n",
      "Epoch: 46, Cost: 8.740809440612793, Accuracy: 0.0\n",
      "Epoch: 47, Cost: 8.656922340393066, Accuracy: 0.0\n",
      "Epoch: 48, Cost: 8.54517936706543, Accuracy: 0.0\n",
      "Epoch: 49, Cost: 8.429008483886719, Accuracy: 0.0\n",
      "Epoch: 50, Cost: 8.297816276550293, Accuracy: 0.0\n",
      "Epoch: 51, Cost: 8.183402061462402, Accuracy: 0.0\n",
      "Epoch: 52, Cost: 8.074390411376953, Accuracy: 0.0\n",
      "Epoch: 53, Cost: 7.9619035720825195, Accuracy: 0.0\n",
      "Epoch: 54, Cost: 7.853233814239502, Accuracy: 0.0\n",
      "Epoch: 55, Cost: 7.757653713226318, Accuracy: 0.0\n",
      "Epoch: 56, Cost: 7.657265663146973, Accuracy: 0.0\n",
      "Epoch: 57, Cost: 7.564793109893799, Accuracy: 0.0\n",
      "Epoch: 58, Cost: 7.474697113037109, Accuracy: 0.0\n",
      "Epoch: 59, Cost: 7.376276969909668, Accuracy: 0.0\n",
      "Epoch: 60, Cost: 7.284928321838379, Accuracy: 0.0\n",
      "Epoch: 61, Cost: 7.201805114746094, Accuracy: 0.0\n",
      "Epoch: 62, Cost: 7.104196548461914, Accuracy: 0.0\n",
      "Epoch: 63, Cost: 7.014685153961182, Accuracy: 0.0\n",
      "Epoch: 64, Cost: 6.923912525177002, Accuracy: 0.0\n",
      "Epoch: 65, Cost: 6.821714401245117, Accuracy: 0.0\n",
      "Epoch: 66, Cost: 6.751551628112793, Accuracy: 0.0\n",
      "Epoch: 67, Cost: 6.686774730682373, Accuracy: 0.0\n",
      "Epoch: 68, Cost: 6.611923694610596, Accuracy: 0.0\n",
      "Epoch: 69, Cost: 6.551997661590576, Accuracy: 0.0\n",
      "Epoch: 70, Cost: 6.477055072784424, Accuracy: 0.0\n",
      "Epoch: 71, Cost: 6.40083646774292, Accuracy: 0.0\n",
      "Epoch: 72, Cost: 6.328733444213867, Accuracy: 0.0\n",
      "Epoch: 73, Cost: 6.276975154876709, Accuracy: 0.0\n",
      "Epoch: 74, Cost: 6.204874038696289, Accuracy: 0.0\n",
      "Epoch: 75, Cost: 6.159332275390625, Accuracy: 0.0\n",
      "Epoch: 76, Cost: 6.108090877532959, Accuracy: 0.0\n",
      "Epoch: 77, Cost: 6.06531286239624, Accuracy: 0.0\n",
      "Epoch: 78, Cost: 6.013056755065918, Accuracy: 0.0\n",
      "Epoch: 79, Cost: 5.9601287841796875, Accuracy: 0.0\n",
      "Epoch: 80, Cost: 5.90446138381958, Accuracy: 0.0\n",
      "Epoch: 81, Cost: 5.864080905914307, Accuracy: 0.0\n",
      "Epoch: 82, Cost: 5.817255020141602, Accuracy: 0.0\n",
      "Epoch: 83, Cost: 5.7686686515808105, Accuracy: 0.0\n",
      "Epoch: 84, Cost: 5.714649677276611, Accuracy: 0.0\n",
      "Epoch: 85, Cost: 5.668938159942627, Accuracy: 0.0\n",
      "Epoch: 86, Cost: 5.617285251617432, Accuracy: 0.0\n",
      "Epoch: 87, Cost: 5.5845160484313965, Accuracy: 0.0\n",
      "Epoch: 88, Cost: 5.542426109313965, Accuracy: 0.0\n",
      "Epoch: 89, Cost: 5.490801811218262, Accuracy: 0.0\n",
      "Epoch: 90, Cost: 5.43186616897583, Accuracy: 0.0\n",
      "Epoch: 91, Cost: 5.376157283782959, Accuracy: 0.0\n",
      "Epoch: 92, Cost: 5.3133368492126465, Accuracy: 0.0\n",
      "Epoch: 93, Cost: 5.257208824157715, Accuracy: 0.0\n",
      "Epoch: 94, Cost: 5.199567794799805, Accuracy: 0.0\n",
      "Epoch: 95, Cost: 5.138437271118164, Accuracy: 0.0\n",
      "Epoch: 96, Cost: 5.080813884735107, Accuracy: 0.0\n",
      "Epoch: 97, Cost: 5.023539066314697, Accuracy: 0.0\n",
      "Epoch: 98, Cost: 4.958072662353516, Accuracy: 0.0\n",
      "Epoch: 99, Cost: 4.900486469268799, Accuracy: 0.0\n",
      "Epoch: 100, Cost: 4.851618766784668, Accuracy: 0.0\n",
      "Epoch: 101, Cost: 4.79329252243042, Accuracy: 0.0\n",
      "Epoch: 102, Cost: 4.738726615905762, Accuracy: 0.0\n",
      "Epoch: 103, Cost: 4.683736324310303, Accuracy: 0.0\n",
      "Epoch: 104, Cost: 4.62675142288208, Accuracy: 0.0\n",
      "Epoch: 105, Cost: 4.571887493133545, Accuracy: 0.0\n",
      "Epoch: 106, Cost: 4.521491050720215, Accuracy: 0.0\n",
      "Epoch: 107, Cost: 4.4706525802612305, Accuracy: 0.0\n",
      "Epoch: 108, Cost: 4.4244160652160645, Accuracy: 0.0\n",
      "Epoch: 109, Cost: 4.377560138702393, Accuracy: 0.0\n",
      "Epoch: 110, Cost: 4.326783657073975, Accuracy: 0.0\n",
      "Epoch: 111, Cost: 4.275433540344238, Accuracy: 0.0\n",
      "Epoch: 112, Cost: 4.215696811676025, Accuracy: 0.0\n",
      "Epoch: 113, Cost: 4.1603569984436035, Accuracy: 0.0\n",
      "Epoch: 114, Cost: 4.1194915771484375, Accuracy: 0.0\n",
      "Epoch: 115, Cost: 4.07912540435791, Accuracy: 0.0\n",
      "Epoch: 116, Cost: 4.034779071807861, Accuracy: 0.0\n",
      "Epoch: 117, Cost: 3.9813554286956787, Accuracy: 0.0\n",
      "Epoch: 118, Cost: 3.922738552093506, Accuracy: 0.0\n",
      "Epoch: 119, Cost: 3.8732519149780273, Accuracy: 0.0\n",
      "Epoch: 120, Cost: 3.828871250152588, Accuracy: 0.0\n",
      "Epoch: 121, Cost: 3.7775094509124756, Accuracy: 0.0\n",
      "Epoch: 122, Cost: 3.7186124324798584, Accuracy: 0.0\n",
      "Epoch: 123, Cost: 3.67082142829895, Accuracy: 0.0\n",
      "Epoch: 124, Cost: 3.617084264755249, Accuracy: 0.0\n",
      "Epoch: 125, Cost: 3.56610107421875, Accuracy: 0.0\n",
      "Epoch: 126, Cost: 3.522967576980591, Accuracy: 0.0\n",
      "Epoch: 127, Cost: 3.466167449951172, Accuracy: 0.0\n",
      "Epoch: 128, Cost: 3.412405252456665, Accuracy: 0.0\n",
      "Epoch: 129, Cost: 3.3709919452667236, Accuracy: 0.0\n",
      "Epoch: 130, Cost: 3.3177080154418945, Accuracy: 0.0\n",
      "Epoch: 131, Cost: 3.278428077697754, Accuracy: 0.006666666828095913\n",
      "Epoch: 132, Cost: 3.228748083114624, Accuracy: 0.006666666828095913\n",
      "Epoch: 133, Cost: 3.169621706008911, Accuracy: 0.006666666828095913\n",
      "Epoch: 134, Cost: 3.11906099319458, Accuracy: 0.006666666828095913\n",
      "Epoch: 135, Cost: 3.0682435035705566, Accuracy: 0.0\n",
      "Epoch: 136, Cost: 3.054241180419922, Accuracy: 0.0\n",
      "Epoch: 137, Cost: 3.052555799484253, Accuracy: 0.0\n",
      "Epoch: 138, Cost: 3.02375864982605, Accuracy: 0.0\n",
      "Epoch: 139, Cost: 2.982255458831787, Accuracy: 0.0\n",
      "Epoch: 140, Cost: 2.933349847793579, Accuracy: 0.0\n",
      "Epoch: 141, Cost: 2.8915350437164307, Accuracy: 0.0\n",
      "Epoch: 142, Cost: 2.8477115631103516, Accuracy: 0.013333333656191826\n",
      "Epoch: 143, Cost: 2.8108766078948975, Accuracy: 0.013333333656191826\n",
      "Epoch: 144, Cost: 2.7702813148498535, Accuracy: 0.0\n",
      "Epoch: 145, Cost: 2.732165813446045, Accuracy: 0.0\n",
      "Epoch: 146, Cost: 2.7007904052734375, Accuracy: 0.006666666828095913\n",
      "Epoch: 147, Cost: 2.660874605178833, Accuracy: 0.013333333656191826\n",
      "Epoch: 148, Cost: 2.6202926635742188, Accuracy: 0.019999999552965164\n",
      "Epoch: 149, Cost: 2.5748865604400635, Accuracy: 0.006666666828095913\n",
      "Epoch: 150, Cost: 2.5436525344848633, Accuracy: 0.0\n",
      "Epoch: 151, Cost: 2.5138418674468994, Accuracy: 0.0\n",
      "Epoch: 152, Cost: 2.4830760955810547, Accuracy: 0.0\n",
      "Epoch: 153, Cost: 2.4417953491210938, Accuracy: 0.0\n",
      "Epoch: 154, Cost: 2.401576519012451, Accuracy: 0.0\n",
      "Epoch: 155, Cost: 2.3526482582092285, Accuracy: 0.006666666828095913\n",
      "Epoch: 156, Cost: 2.3155322074890137, Accuracy: 0.006666666828095913\n",
      "Epoch: 157, Cost: 2.279372215270996, Accuracy: 0.013333333656191826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 158, Cost: 2.2399368286132812, Accuracy: 0.013333333656191826\n",
      "Epoch: 159, Cost: 2.205618143081665, Accuracy: 0.013333333656191826\n",
      "Epoch: 160, Cost: 2.174504041671753, Accuracy: 0.013333333656191826\n",
      "Epoch: 161, Cost: 2.128112316131592, Accuracy: 0.006666666828095913\n",
      "Epoch: 162, Cost: 2.091526746749878, Accuracy: 0.0\n",
      "Epoch: 163, Cost: 2.056020975112915, Accuracy: 0.0\n",
      "Epoch: 164, Cost: 2.015434503555298, Accuracy: 0.006666666828095913\n",
      "Epoch: 165, Cost: 1.975797176361084, Accuracy: 0.006666666828095913\n",
      "Epoch: 166, Cost: 1.9357084035873413, Accuracy: 0.006666666828095913\n",
      "Epoch: 167, Cost: 1.8958516120910645, Accuracy: 0.006666666828095913\n",
      "Epoch: 168, Cost: 1.860059380531311, Accuracy: 0.013333333656191826\n",
      "Epoch: 169, Cost: 1.8192285299301147, Accuracy: 0.046666666865348816\n",
      "Epoch: 170, Cost: 1.7926108837127686, Accuracy: 0.03333333507180214\n",
      "Epoch: 171, Cost: 1.7642525434494019, Accuracy: 0.013333333656191826\n",
      "Epoch: 172, Cost: 1.7278153896331787, Accuracy: 0.006666666828095913\n",
      "Epoch: 173, Cost: 1.6914790868759155, Accuracy: 0.019999999552965164\n",
      "Epoch: 174, Cost: 1.6572555303573608, Accuracy: 0.02666666731238365\n",
      "Epoch: 175, Cost: 1.629830241203308, Accuracy: 0.019999999552965164\n",
      "Epoch: 176, Cost: 1.5963560342788696, Accuracy: 0.013333333656191826\n",
      "Epoch: 177, Cost: 1.5671703815460205, Accuracy: 0.006666666828095913\n",
      "Epoch: 178, Cost: 1.5423431396484375, Accuracy: 0.006666666828095913\n",
      "Epoch: 179, Cost: 1.522244930267334, Accuracy: 0.006666666828095913\n",
      "Epoch: 180, Cost: 1.4964134693145752, Accuracy: 0.019999999552965164\n",
      "Epoch: 181, Cost: 1.4710038900375366, Accuracy: 0.02666666731238365\n",
      "Epoch: 182, Cost: 1.446872591972351, Accuracy: 0.02666666731238365\n",
      "Epoch: 183, Cost: 1.4254308938980103, Accuracy: 0.019999999552965164\n",
      "Epoch: 184, Cost: 1.4045029878616333, Accuracy: 0.013333333656191826\n",
      "Epoch: 185, Cost: 1.382939100265503, Accuracy: 0.019999999552965164\n",
      "Epoch: 186, Cost: 1.3639068603515625, Accuracy: 0.046666666865348816\n",
      "Epoch: 187, Cost: 1.3448773622512817, Accuracy: 0.03333333507180214\n",
      "Epoch: 188, Cost: 1.330566644668579, Accuracy: 0.019999999552965164\n",
      "Epoch: 189, Cost: 1.3145170211791992, Accuracy: 0.013333333656191826\n",
      "Epoch: 190, Cost: 1.305263638496399, Accuracy: 0.013333333656191826\n",
      "Epoch: 191, Cost: 1.299710750579834, Accuracy: 0.013333333656191826\n",
      "Epoch: 192, Cost: 1.2894163131713867, Accuracy: 0.006666666828095913\n",
      "Epoch: 193, Cost: 1.2799668312072754, Accuracy: 0.019999999552965164\n",
      "Epoch: 194, Cost: 1.2699867486953735, Accuracy: 0.019999999552965164\n",
      "Epoch: 195, Cost: 1.256184697151184, Accuracy: 0.019999999552965164\n",
      "Epoch: 196, Cost: 1.2424578666687012, Accuracy: 0.006666666828095913\n",
      "Epoch: 197, Cost: 1.2329251766204834, Accuracy: 0.013333333656191826\n",
      "Epoch: 198, Cost: 1.2220407724380493, Accuracy: 0.013333333656191826\n",
      "Epoch: 199, Cost: 1.2117588520050049, Accuracy: 0.013333333656191826\n",
      "Epoch: 200, Cost: 1.199747085571289, Accuracy: 0.013333333656191826\n",
      "Epoch: 201, Cost: 1.1895861625671387, Accuracy: 0.02666666731238365\n",
      "Epoch: 202, Cost: 1.1800284385681152, Accuracy: 0.046666666865348816\n",
      "Epoch: 203, Cost: 1.1693354845046997, Accuracy: 0.03333333507180214\n",
      "Epoch: 204, Cost: 1.1640387773513794, Accuracy: 0.03999999910593033\n",
      "Epoch: 205, Cost: 1.1590588092803955, Accuracy: 0.02666666731238365\n",
      "Epoch: 206, Cost: 1.152819037437439, Accuracy: 0.02666666731238365\n",
      "Epoch: 207, Cost: 1.1558432579040527, Accuracy: 0.03333333507180214\n",
      "Epoch: 208, Cost: 1.1549031734466553, Accuracy: 0.02666666731238365\n",
      "Epoch: 209, Cost: 1.1505094766616821, Accuracy: 0.02666666731238365\n",
      "Epoch: 210, Cost: 1.1466467380523682, Accuracy: 0.013333333656191826\n",
      "Epoch: 211, Cost: 1.1426254510879517, Accuracy: 0.013333333656191826\n",
      "Epoch: 212, Cost: 1.1394383907318115, Accuracy: 0.03999999910593033\n",
      "Epoch: 213, Cost: 1.1375571489334106, Accuracy: 0.03999999910593033\n",
      "Epoch: 214, Cost: 1.137848973274231, Accuracy: 0.046666666865348816\n",
      "Epoch: 215, Cost: 1.1391973495483398, Accuracy: 0.03333333507180214\n",
      "Epoch: 216, Cost: 1.1403791904449463, Accuracy: 0.03333333507180214\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "rmse=[]\n",
    "rmse_motor=[]\n",
    "for k in range(0,K):\n",
    "    print(\"CROSS VALIDATION ITERATION\",k)\n",
    "    # Initializing session    \n",
    "    sess = tf.Session()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    train_features=[]\n",
    "    train_features2=[]\n",
    "    test_features=[]\n",
    "    train_target=[]\n",
    "    test_target=[]\n",
    "    \n",
    "    \n",
    "    #Define saver object to save NN\n",
    "    locals()[\"saver_\"+str(k)]=tf.train.Saver()\n",
    "    train_features= effortFeatures[0:k*set_length,0:D]\n",
    "    train_features=np.concatenate((train_features,effortFeatures[(k+1)*set_length:K*set_length,0:D]))\n",
    "    \n",
    "    test_features= effortFeatures[k*set_length:(k+1)*set_length,0:D]\n",
    "                               \n",
    "\n",
    "    train_target= -effortFeatures[0:k*set_length,D]\n",
    "    train_target=np.concatenate((train_target,-effortFeatures[(k+1)*set_length:K*set_length,D]))\n",
    "    \n",
    "    test_target=-effortFeatures[k*set_length:(k+1)*set_length,D]\n",
    "                           \n",
    "                               \n",
    "    #Reshaping data\n",
    "    train_features=np.asarray(train_features)\n",
    "    train_features=np.reshape(train_features,[len(train_features),D])\n",
    "    test_features=np.asarray(test_features)\n",
    "    test_features=np.reshape(test_features,[len(test_features),D])\n",
    "    train_target=np.asarray(train_target)\n",
    "    train_target=np.reshape(train_target,[len(train_target),1])\n",
    "    test_target=np.asarray(test_target) \n",
    "    test_target=np.reshape(test_target,[len(test_target),1]) \n",
    "    \n",
    "    #Regularize to radius\n",
    "    train_target=train_target*radius\n",
    "    test_target=test_target*radius\n",
    "\n",
    "    costs = []\n",
    "    accuracies=[]\n",
    "\n",
    "    #setting up training\n",
    "    #for epoch in range(epochNumber):\n",
    "    acc=0\n",
    "    epoch=0\n",
    "    while(acc<desiredAccuracy or epoch<epochNumber):\n",
    "        total_batch=int(len(train_features)/batch_size)\n",
    "        for i in range(total_batch):   \n",
    "            inp_data= train_features[i*batch_size:(i+1)*batch_size,:]\n",
    "            out_data= train_target[i*batch_size:(i+1)*batch_size,:]\n",
    "            _,acc,c = sess.run([optimizer,accuracy,cost],feed_dict={Xin: inp_data, Ytarget: out_data})\n",
    "        print(\"Epoch: {}, Cost: {}, Accuracy: {}\".format(epoch,c,acc))\n",
    "        costs.append(c)\n",
    "        accuracies.append(acc)\n",
    "        epoch=epoch+1\n",
    "\n",
    "    #Save session after training\n",
    "    saver_path=locals()[\"saver_\"+str(k)].save(sess,'results3/NN_tension_pred_saver',global_step=k)\n",
    "\n",
    "    print (\"\\nTraining complete!\")\n",
    "\n",
    "    predicted=[]\n",
    "    test_batch=int(len(test_features)/batch_size)\n",
    "\n",
    "    #Test prediction with test dataset\n",
    "    for j in range (test_batch):    \n",
    "        inp_data = test_features[j*batch_size:(j+1)*batch_size,:]\n",
    "        pred = sess.run(y_, feed_dict={Xin: inp_data})\n",
    "        predicted=np.append(predicted,pred)\n",
    "\n",
    "    #Plot prediction against real data from test dataset\n",
    "    x=np.arange(0,len(test_target[0:(batch_size*test_batch)]))\n",
    "    y_pred= predicted\n",
    "    #invert because i invert the y_tar value when training\n",
    "    y_tar= test_target[0:(batch_size*test_batch)]\n",
    "    #y3=-test_features[:,21]*0.0175/0.008\n",
    "    y_motor=effort[k*set_length:k*set_length+(batch_size*test_batch),21]/0.0155*2.5\n",
    "\n",
    "    fig= plt.figure(figsize=(20,10))\n",
    "    ax1=plt.subplot(211)\n",
    "    ax1.plot(x,y_pred,'b',label='Predictions')\n",
    "    ax1.plot(x,y_tar,'r',label='Targets')\n",
    "    ax1.plot(x,y_motor,'g',label='Motor 22')\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.title('Prediction using 24 motor torques as features')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot cost function and accuracy\n",
    "    x=np.arange(0,epochNumber)\n",
    "    y= costs\n",
    "\n",
    "    fig= plt.figure(figsize=(10,10))\n",
    "    ax1=plt.subplot(211)\n",
    "    ax1.plot(x,y,'b',label='Cost')\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.title('Cost Fuction during Training')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Plot cost function and accuracy\n",
    "    x=np.arange(0,epochNumber)\n",
    "    y= accuracies\n",
    "\n",
    "    fig= plt.figure(figsize=(10,10))\n",
    "    ax1=plt.subplot(211)\n",
    "    ax1.plot(x,y,'r',label='Accuracy')\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "\n",
    "    plt.title('Accuracy during Training')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    rmse=np.append(rmse,sqrt(mean_squared_error(y_pred,y_tar)))\n",
    "    rmse_motor=np.append(rmse_motor,sqrt(mean_squared_error(y_motor,y_tar)))\n",
    "    print(\"And now costs for iteration \", k)\n",
    "    print(costs)\n",
    "    print(\"And now accuracies for iteration \", k)\n",
    "    print(accuracies)\n",
    "    print(\"Root mean squared error of prediction\",rmse[k])\n",
    "    print(\"Root mean squared error of motor\",rmse_motor[k])\n",
    "\n",
    "    sess.close()\n",
    "print(rmse)\n",
    "print(rmse_motor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ADAM\n",
    "#batch_size=800, lr=0.0001, K=10, H=4*D, epoch_number=100 \n",
    "#[ 0.50253549  0.43781634  0.32040607  0.32329862  0.36412889  0.35812448\n",
    "#  0.27207648  0.43003502  0.32162693  0.40849026]\n",
    "#[ 0.70579643  0.64966416  0.50766536  0.50934368  0.59895063  0.61945065\n",
    "#  0.48574498  0.67014488  0.48967202  0.61927773]\n",
    "#\n",
    "#batch_size=100, lr=0.0003, K=10, H=5*D, epoch_number=100\n",
    "#[ 0.46218572  0.44220819  0.39805842  0.41903569  0.3219875   0.3759407\n",
    "#  0.35630052  0.46268236  0.37257996  0.54600813]\n",
    "#[ 0.70500703  0.64769705  0.50612557  0.50971187  0.59769723  0.61814086\n",
    "#  0.48427049  0.66855334  0.4923125   0.61763899]\n",
    "\n",
    "\n",
    "#batch_size=500, lr=0.0003, K=10, H=5*D, epoch_number=100\n",
    "#[ 0.45112949  0.41166623  0.3488705   0.33144462  0.30662881  0.37951444\n",
    "#  0.26394875  0.4951899   0.29257544  0.48440297]\n",
    "#[ 0.70496692  0.64826301  0.50657057  0.50966813  0.59798115  0.61856779\n",
    "#  0.48468212  0.66901232  0.49050164  0.61812131]\n",
    "\n",
    "#batch_size=500, lr=0.0003, K=10, H=5*D, epoch_number=300\n",
    "#[ 0.46730755  0.39162348  0.33621428  0.35487699  0.33413626  0.4338099\n",
    "#  0.338667    0.46846256  0.3382114   0.55634947]\n",
    "#[ 0.70496692  0.64826301  0.50657057  0.50966813  0.59798115  0.61856779\n",
    "#  0.48468212  0.66901232  0.49050164  0.61812131]\n",
    "\n",
    "##batch_size=200, lr=0.00005, K=10, H=15*D, epoch_number=100\n",
    "#[ 0.43956565  0.39496273  0.30556959  0.32533406  0.33855176  0.37363014\n",
    "#  0.28639368  0.46719669  0.31364863  0.42626489]\n",
    "#[ 0.70492231  0.64797778  0.50634846  0.50969095  0.59783157  0.61834494\n",
    "#  0.48447538  0.66878822  0.49126617  0.6178877 ]\n",
    "\n",
    "#batch_size=200, lr=0.00005, K=10, H=20*D, epoch_number=100\n",
    "#[ 0.45373054  0.39110336  0.32326501  0.33262925  0.35039483  0.40166448\n",
    "#  0.25592091  0.4547832   0.30091716  0.44751162]\n",
    "#[ 0.70492231  0.64797778  0.50634846  0.50969095  0.59783157  0.61834494\n",
    "#  0.48447538  0.66878822  0.49126617  0.6178877 ]\n",
    "\n",
    "\n",
    "#MINI BATCH GD\n",
    "#batch_size=100, lr=0.00001, K=10, H=15*D, epoch_number=200\n",
    "#[ 0.48721713,  0.43863498,  0.33641318,  0.33274214,  0.36547824,  0.35071493,\n",
    "#  0.27092393,  0.42188746,  0.33233472,  0.42640026]\n",
    "#[ 0.70500703  0.64769705  0.50612557  0.50971187  0.59769723  0.61814086\n",
    "#  0.48427049  0.66855334  0.4923125   0.61763899]\n",
    "\n",
    "#batch_size=50, lr=0.00001, K=10, H=15*D, epoch_number=100\n",
    "#[ 0.48799102  0.45342051  0.33284949  0.32443756  0.35144045  0.34472716\n",
    "#  0.28091073  0.42006393  0.33825867  0.42149133]\n",
    "#[ 0.70509088  0.64755575  0.50601392  0.50971825  0.59763682  0.61804062\n",
    "#  0.48416542  0.66842852  0.49275822  0.61751091]\n",
    "\n",
    "\n",
    "#batch_size=50, lr=0.00001, K=10, H=18*D, epoch_number=100\n",
    "#[ 0.49460297  0.45634525  0.32905642  0.32753956  0.35563032  0.35412292\n",
    "#  0.27835449  0.42779808  0.33047787  0.41999861]\n",
    "#[ 0.70509088  0.64755575  0.50601392  0.50971825  0.59763682  0.61804062\n",
    "#  0.48416542  0.66842852  0.49275822  0.61751091]\n",
    "\n",
    "#batch_size=50, lr=0.00001, K=10, H=10*D, epoch_number=100\n",
    "#[ 0.48540827  0.44815957  0.33539943  0.33215633  0.37294727  0.34524683\n",
    "#  0.26910294  0.42505782  0.34029048  0.42350589]\n",
    "#[ 0.70509088  0.64755575  0.50601392  0.50971825  0.59763682  0.61804062\n",
    "#  0.48416542  0.66842852  0.49275822  0.61751091]\n",
    "\n",
    "\n",
    "#batch_size=50, lr=0.00001, K=7, H=15*D, epoch_number=100\n",
    "[ 0.49522226  0.3746474   0.35643217  0.33235772  0.33975522  0.34873753\n",
    "  0.41965114]\n",
    "[ 0.69181523  0.56609945  0.54216891  0.58965845  0.54702666  0.5808668\n",
    "  0.59657655]\n",
    "\n",
    "#batch_size=50, lr=0.000005, K=10, H=15*D, epoch_number=200\n",
    "[ 0.48740707  0.46531014  0.33256863  0.33262834  0.35801402  0.34543959\n",
    "  0.28328477  0.41193634  0.33103235  0.42206953]\n",
    "[ 0.70509088  0.64755575  0.50601392  0.50971825  0.59763682  0.61804062\n",
    "  0.48416542  0.66842852  0.49275822  0.61751091]\n",
    "\n",
    "\n",
    "GOOD ONE\n",
    "#batch_size=50, lr=0.000005, K=10, H=15*D, epoch_number=300\n",
    "# [0.47976632  0.45696714  0.33366897  0.3296227   0.35630887  0.33606939\n",
    "#  0.28331008  0.41057156  0.32990863  0.42158835]\n",
    "#[ 0.70509088  0.64755575  0.50601392  0.50971825  0.59763682  0.61804062\n",
    "#  0.48416542  0.66842852  0.49275822  0.61751091]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ciao=[ 0.70509088,  0.64755575 , 0.50601392 , 0.50971825 , 0.59763682,  0.61804062,\n",
    "  0.48416542 , 0.66842852 , 0.49275822 , 0.61751091]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584691931\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(ciao))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1127580, 25)\n",
      "(1127580, 21)\n",
      "(1127580, 2)\n",
      "(1127580, 23)\n"
     ]
    }
   ],
   "source": [
    "effort=dataset1.get('effort')\n",
    "#effort=np.concatenate((effort,dataset2.get('effort')))\n",
    "#effort=np.concatenate((effort,dataset3.get('effort')))\n",
    "print(effort.shape)\n",
    "effort1=effort[:,0:21]\n",
    "effort2=effort[:,21:D]\n",
    "effort=np.concatenate((effort1,effort2),axis=1)\n",
    "print(effort1.shape)\n",
    "print(effort2.shape)\n",
    "print(effort.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "think about SHIFTING UP cause the relu is only from 0 up!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
